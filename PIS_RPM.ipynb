{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b87d1449",
   "metadata": {},
   "source": [
    "# Env pycaret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408b15b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaret.classification import *\n",
    "from pycaret.datasets import get_data\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30df040",
   "metadata": {},
   "source": [
    "#### 1.1 data info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0302b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_selected = pd.read_csv(\"train.csv\", encoding='gb18030')\n",
    "print(data_selected.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8594f7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=seed)  \n",
    "for train_index, test_index in split.split(data_selected, data_selected['y']):\n",
    "    validation_indices = test_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d514d9",
   "metadata": {},
   "source": [
    "#### 1.2 set env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e08dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_env = setup(data = data_selected, target = 'y' ,transformation = True, normalize = True,\n",
    "                use_gpu = False, fold = 5,train_size = 0.8, session_id=seed,  \n",
    "                data_split_shuffle = True, data_split_stratify = True,   \n",
    "                categorical_imputation = 'mode', numeric_imputation = 'median')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8d96d3",
   "metadata": {},
   "source": [
    "#### 1.3 model train and select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9768894f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "\n",
    "    'AdaBoostClassifier': {\n",
    "        'n_estimators': [10,10000],\n",
    "        'learning_rate': [0.1, 1],\n",
    "    },\n",
    "    \n",
    "    'BaggingClassifier': {\n",
    "        'n_estimators': [10, 100, 1000, 10000],\n",
    "        'max_samples' : [1, 10, 100, 1000],\n",
    "    },\n",
    "    \n",
    "    'GradientBoostingClassifier': {\n",
    "        'n_estimators': [10, 100, 1000, 10000],\n",
    "        'learning_rate': [0.001, 0.01, 0.1, 1]\n",
    "    },\n",
    "    \n",
    "    'LassoRegression':{\n",
    "        'alpha': [0.01, 0.1, 0.5, 1]\n",
    "    },\n",
    "    \n",
    "    'RandomForestClassifier': {\n",
    "        'n_estimators': [10, 100],\n",
    "        'max_features': ['auto'],\n",
    "        'max_depth': [5, 10],\n",
    "        'criterion': ['gini']\n",
    "    },\n",
    "    \n",
    "    'XGBClassifier': {\n",
    "        'max_depth': [2, 3],\n",
    "        'n_estimators': [100, 1000],\n",
    "        'learning_rate': [0.1, 0.2],\n",
    "        'eval_metric' : ['mlogloss']\n",
    "    },\n",
    "    \n",
    "    'ExtraTreesClassifier': {\n",
    "        'n_estimators': [100, 1000, 10000],\n",
    "        'criterion' : ['gini', 'entropy'],\n",
    "        'max_features': ['auto', 'log2'],\n",
    "        'max_depth': [2, 5, 10, 20, 50, 100]\n",
    "    },\n",
    "    \n",
    "    'LogisticRegression': {\n",
    "        'penalty' : ['l1', 'l2'],\n",
    "        'C' : [0.1, 1, 10, 100, 200, 500, 1000]\n",
    "    },\n",
    "    \n",
    "    'PassiveAggressiveClassifier': {\n",
    "        'C' : [0.0001, 0.0003, 0.001, 0.003, 0.01],\n",
    "        'loss': ['hinge', 'squared_hinge'],\n",
    "        'n_iter_no_change': [5, 10, 30, 100, 300]\n",
    "    },\n",
    "    \n",
    "    'SGDClassifier': {\n",
    "        'loss': ['modified_huber'],\n",
    "        'alpha': [0.01, 0.1, 0.5, 1],\n",
    "        'penalty': ['l2', 'l1', None]\n",
    "    },\n",
    "    \n",
    "    'Perceptron': {\n",
    "        'alpha': [0.0001, 0.001, 0.01],\n",
    "        'penalty': ['l2', 'l1', None]\n",
    "    },\n",
    "    \n",
    "    'BernoulliNB': {\n",
    "        'alpha': np.linspace(0.1,1,10),\n",
    "        'binarize': [0, None],\n",
    "        'fit_prior': [True, False]\n",
    "    },\n",
    "    \n",
    "    'GaussianNB': {},\n",
    "\n",
    "    'KNeighborsClassifier': {\n",
    "        'n_neighbors' : [3, 5, 10, 20],\n",
    "        'leaf_size' : [2, 5, 10, 20],\n",
    "        'p' : [0.5, 1, 2, 5],\n",
    "        'weights' : ['uniform', 'distance'],\n",
    "        'algorithm' : ['auto', 'ball_tree', 'kd_tree', 'brute']\n",
    "    },\n",
    "    \n",
    "    'LinearSVC': {\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'loss': ['hinge', 'squared_hinge'],\n",
    "        'C': [0.1, 0.5, 1, 5, 10]\n",
    "    },\n",
    "    \n",
    "    'DecisionTreeClassifier': {\n",
    "        'criterion': ['entropy','gini'],\n",
    "        'splitter' : ['random', 'best'],\n",
    "        'max_depth':[1, 2, 3, 5, 6, 8, 10],\n",
    "        'min_samples_leaf':[1,2,3,4,5,6,7,8,9,10]\n",
    "    },\n",
    "      \n",
    "    'LinearDiscriminantAnalysis': {\n",
    "        'solver': ['svd', 'lsqr', 'eigen'],\n",
    "        'tol': [0.00001,0.0001,0.0003]\n",
    "    },\n",
    "    \n",
    "    'QuadraticDiscriminantAnalysis': {\n",
    "        'reg_param': [0.1, 0.5, 0.7, 0.9],\n",
    "        'tol': [0.00001,0.0001,0.0003]\n",
    "    },\n",
    "    \n",
    "    'MLPClassifier': {\n",
    "        'solver': ['lbfgs', 'adam'],\n",
    "        'learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "        'hidden_layer_sizes': [(10, 7, 3), (30, 20, 12), (50, 35, 25), (70, 50, 35)],\n",
    "        'activation': ['identity', 'logistic', 'tanh', 'relu']\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c0f440",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy import stats \n",
    "\n",
    "def string_match_(params,string_dict):\n",
    "    result=[]\n",
    "    list_key=re.split(r'[;,\\s()]\\s*',string_dict)\n",
    "    for key, value in params.items():\n",
    "        for string_o in list_key:\n",
    "            if key in string_o:\n",
    "                result.append(string_o)\n",
    "    return result\n",
    "\n",
    "def bootstrap_auc(y, pred, classes, bootstraps = 100): \n",
    "    fold_size = len(y)\n",
    "    statistics = np.zeros((len(classes), bootstraps))\n",
    "    for c in range(len(classes)):\n",
    "        dataframe = pd.DataFrame(columns=['y', 'pred'])\n",
    "        dataframe.loc[:, 'y'] = y[:]\n",
    "        dataframe.loc[:, 'pred'] = pred[:]\n",
    "        df_pos = dataframe[dataframe.y == 1]\n",
    "        df_neg = dataframe[dataframe.y == 0]\n",
    "        prevalence = len(df_pos) / len(dataframe)\n",
    "        for i in range(bootstraps):\n",
    "            pos_sample = df_pos.sample(n = int(fold_size * prevalence), replace=True)\n",
    "            neg_sample = df_neg.sample(n = int(fold_size * (1-prevalence)), replace=True)\n",
    "            y_sample = np.concatenate([pos_sample.y.values, neg_sample.y.values])\n",
    "            pred_sample = np.concatenate([pos_sample.pred.values, neg_sample.pred.values])\n",
    "            score = roc_auc_score(y_sample, pred_sample)\n",
    "            statistics[c][i] = score\n",
    "        CI = stats.t.interval(alpha=0.95, df=len(statistics[c]) - 1, loc=np.mean(statistics[c]), \n",
    "                              scale=stats.sem(statistics[c]))  #stats.sem / np.std\n",
    "    return CI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69ffad3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_all = []\n",
    "val_all1 = []\n",
    "val_all2 = []\n",
    "data_proba = {}\n",
    "val_proba1 = {}\n",
    "val_proba2 = {}\n",
    "no_grid = ['nb']\n",
    "para_model = [['lr','LogisticRegression',],         #- Logistic Regression\n",
    "             ['nb','GaussianNB'],                   #- Naive Bayes                    \n",
    "             ['mlp','MLPClassifier'],               #- MLP Classifier\n",
    "             ['rf','RandomForestClassifier'],       #- Random Forest Classifier\n",
    "             ['ada','AdaBoostClassifier'],          #- Ada Boost Classifier\n",
    "             ['xgboost','XGBClassifier'],           #- Extreme Gradient Boosting       \n",
    "             ['lda','LinearDiscriminantAnalysis'],  #- Linear Discriminant Analysis\n",
    "             ['knn','KNeighborsClassifier']         #- K Neighbors Classifier\n",
    "             ]      \n",
    "for single_model,single_para in para_model:\n",
    "    model = create_model(single_model)\n",
    "    params = model_params[single_para]\n",
    "    if single_model in no_grid:\n",
    "        tuned_model = tune_model(model)\n",
    "    else:\n",
    "        tuned_model = tune_model(model, custom_grid = params, optimize='AUC',   # optimize：Accuracy/AUC/F1/Recall/Precision\n",
    "                                 search_library = 'scikit-learn', search_algorithm = 'grid',choose_better = True, n_iter = 50)\n",
    "    tuned_model_result = pull()\n",
    "    pred = predict_model(tuned_model, probability_threshold = 0.5)\n",
    "    tuned_model_result_2 = pull()\n",
    "    data_proba[single_para] = [1-a if list(pred['prediction_label'])[i]==0 \n",
    "                               else a for i,a in enumerate(list(pred['prediction_score'])) ] \n",
    "    data_proba[single_para+\"_y\"] = list(pred['y'])\n",
    "    \n",
    "    data_single = [single_para,tuned_model_result_2['AUC'],\n",
    "                   bootstrap_auc(np.array(data_proba[single_para+\"_y\"]),np.array(data_proba[single_para]), \n",
    "                                 [single_para]),string_match_(params,str(tuned_model))] \n",
    "    data_all.append(data_single)\n",
    "    \n",
    "    # finalize model :\n",
    "    final_model = finalize_model(tuned_model)\n",
    "    \n",
    "    \n",
    "    # 外部验证：\n",
    "    test1 = pd.read_csv(\"test.csv\", encoding='gb18030')\n",
    "    val_pred1 = predict_model(final_model, data = test1)\n",
    "    final_model_result_1 = pull()\n",
    "    val_proba1[single_para] = [1-a if list(val_pred1['prediction_label'])[i]==0 \n",
    "                              else a for i,a in enumerate(list(val_pred1['prediction_score'])) ] \n",
    "    val_proba1[single_para+\"_y\"] = list(val_pred1['y'])\n",
    "    final_single1 = [single_para, final_model_result_1['AUC'],\n",
    "                    bootstrap_auc(np.array(val_proba1[single_para+\"_y\"]),np.array(val_proba1[single_para]), [single_para])] \n",
    "    val_all1.append(final_single1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672c50d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  内部验证结果：\n",
    "df = pd.DataFrame(data_all,columns=['Models','AUC_M','AUC','best_hyper'], dtype=float)    \n",
    "df.to_csv('C:/Users/mialu/Desktop/AUC_ivs.csv')      \n",
    "df_pro = pd.DataFrame(data_proba)   \n",
    "df_pro.to_csv('C:/Users/mialu/Desktop/prob_ivs.csv')  \n",
    "\n",
    "#  外部验证结果：\n",
    "val1 = pd.DataFrame(val_all1,columns=['Models','AUC_M','AUC'], dtype=float)   \n",
    "val1.to_csv('C:/Users/mialu/Desktop/AUC_val1.csv')     \n",
    "val_pro1 = pd.DataFrame(val_proba1)   \n",
    "val_pro1.to_csv('C:/Users/mialu/Desktop/prob_val1.csv') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pycaret",
   "language": "python",
   "name": "pycaret"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
